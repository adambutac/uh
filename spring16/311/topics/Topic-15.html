<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>ICS 311 #15: Amortized Analysis </title>
</head>

<body>

<hr><h1><a href="../index.html">ICS 311</a> #15: Amortized Analysis </h1><hr> 
<!-- Spring 2014 version -->

<!-- ------------------------------------------------------------ -->
<h2>Outline</h2>

<ol>
  <li>Amortized Analysis: The General Idea </li>
  <li>Multipop Example </li>
  <li>Aggregate Analysis </li>
  <li>Accounting Method</li>
  <li>Potential Method - <i> Optional for 2015, but it's the most powerful method </i> </li>
  <li>Dynamic Table Example (first look)</li> 
  <li>Other Examples</li>
</ol>

<h2>Readings and Screencasts</h2>
<ul>
  <li>Chapter 17 of Cormen et al. (2015: through 17.2, plus 17.4.1) </li>  
  <li>Screencasts <a href="http://youtu.be/L-i9ZcbROHc">15A</a>, 
       <a href="http://youtu.be/iy-WhloN6vA">15B</a> - 
       (also in Laulima and iTunesU)</li>
</ul>


<!-- ------------------------------------------------------------ -->
<hr><h2> Amortized Analysis: The General Idea </h2>

<p>We have already used <em>aggregate</em> analysis several times in this course. For example, when
analyzing the BFS and DFS procedures, instead of trying to figure out how many times their inner
loops</p>

<blockquote> 
<tt>for each <i>v</i> &in; G.Adj[<i>u</i>]</tt> 
</blockquote> 

<p> execute (which depends on the degree of the vertex being processed), we realized that no matter
how the edges are distributed, there are at most |<i>E</i>| edges, so in aggregate across all calls
the loops will execute |<i>E</i>| times.</p>

<p>But that analysis concerned itself only with the complexity of a single operation. In practice a
given data structure will have associated with it several operations, and they may be applied many
times with varying frequency.</p>

<p>Sometimes a given operation is designed to pay a larger cost than would otherwise be necessary to
enable other operations to be lower cost.</p>

<p><i>Example:</i> Red-black tree insertion. We pay a greater cost for balancing so that future searches
will remain O(lg <i>n</i>).</p> 

<p><i>Another example:</i> Java Hashtable.</p>
<ul>
  <li>These grow dynamically when a specified load factor is exceeded.</li>
  <li> Copying into a new table is expensive, but copying is infrequent and table growth makes
       access operations faster.</li>
</ul> 

<p>It is "fairer" to average the cost of the more expensive operations across the entire mix of
operations, as all operations benefit from this cost.</p> 

<p>Here, "average" means average cost in the worst case (no probability is involved,
which greatly simplifies the analysis).</p> 

<p>We will look at three methods. The notes below use Stacks with Multipop to illustrate the
methods. See the text for binary counter examples.</p>

<p>(We have already seen examples of aggregate analysis throughout the semseter. We will see examples
of amortized analysis later in the semester.)</p> 

<!-- ------------------------------------------------------------ -->
<hr><h2> Multipop Example </h2>

<p>We already have the stack operations:</p>
<ul>
  <li><tt>Push(<i>S, x</i>)</tt>: O(1) each call, so O(<i>n</i>) for any sequence of <i>n</i> operations.</li>
  <li><tt>Pop(<i>S</i>)</tt>: O(1) each call, so O(<i>n</i>) for any sequence of <i>n</i> operations.</li>
</ul> 

<p>Suppose we add a <tt>Multipop</tt> (this is a generalization of <tt>ClearStack</tt>, which
empties the stack): </p>
<img src="Topic-15/Fig-17-1-multipop.jpg" align="right" border="1">
<img src="Topic-15/pseudocode-multipop.jpg">

<p>The example shows a <tt>Multipop(S,4)</tt> followed by another where <i>k</i> &ge; 2.</p>

<p>Running time of <tt>Multipop</tt>: 
<ul>
  <li>Linear in number of <tt>Pop</tt> operations (one per loop iteration)</li>
  <li>Number of iterations of <tt>while</tt> loop is min(<i>s</i>, <i>k</i>), where <i>s</i> =
  number of items on the stack</li>
  <li>Therefore, total cost = min(<i>s</i>, <i>k</i>). </li> 
</ul>

<p>What is the worst case of a sequence of <i>n</i> <tt>Push</tt>, <tt>Pop</tt> and
<tt>Multipop</tt> operations?</p> 
<p>Using our existing methods of analysis we can identify a <i>loose bound:</i>: 
<ul>
  <li> The most expensive operation is <tt>Multipop</tt>, potentially O(<i>n</i>).</li>
  <li> Therefore, potentially O(<i>n<sup>2</sup></i>) over <i>n</i> operations.</li>
</ul> 

<!-- ------------------------------------------------------------ -->
<hr><h2> Aggregate Analysis </h2>

<p>We can tighten this loose bound by aggregating the analysis over all <i>n</i> operations:</p> 
<ul>
 <li>Each object can only be popped once per time that it is pushed.</li>
 <li>There are at most <i>n</i> <tt>Push</tt>es, so at most n <tt>Pop</tt>s, including those in
     <tt>Multipop</tt></li> 
 <li>Therefore, total cost = O(<i>n</i>)</li>
 <li>Averaging over the <i>n</i> operations we get O(1) per operation.</li>
</ul>

<p>This analysis shows O(1) per operation on average in a sequence of <i>n</i> operations without
using any probabilities of operations.</p>

<p>See text for example of aggregate analysis of binary counting. An example of aggregate analysis
of dynamic tables is at the end of these notes.</p>

<p>Some of our previous analyses with indicator random variables have been a form of aggregate
analysis, e.g., our analysis of the expected number of inversions in sorting, <a
 href="Topic-05.html">Topic 5 Notes</a>.</p>

<p>Aggregate analysis treats all operations equally. The next two methods let us give different
operations different costs, so are more flexible. </p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Accounting Method </h2>

<h4>Metaphor:</h4> 
<ul>
  <li>View the computer as a coin operated appliance that requires one <i>cyber-dollar</i> (CY$) per
      basic operation.</li> 
  <li>The banks are wary of making loans these days, so when an operation is to be performed we must
      have enough cyber-dollars available to pay for it. </li>
  <li>We are permitted to charge some operations more than they actually cost so we can save enough
      to pay for the more expensive operations.</li>
</ul>

<p><b>Amortized cost</b> = amount we charge each operation.</p>

<p>This differs from aggregate analysis:</p>
<ul>
  <li>In aggregate analysis, all operations have the same cost.</li>
  <li>In the accounting method, different operations can have different costs.</li>
</ul> 

<p>When an operation is overcharged (amortized cost &gt; actual cost), the difference is associated
with <i>specific objects</i> in the data structure as <i>credit</i>. </p>

<p>We use this credit to pay for operations whose actual cost &gt; amortized cost.</p>

<p>The credit must never be negative. Otherwise the amortized cost may not be an upper bound on
actual cost for some sequences.</p> 

<p>Let </p>
<ul>
  <li><i>c<sub>i</sub></i> = actual cost of <i>i</i>th operation. </li>
  <li><i>&ccirc;<sub>i</sub></i> = amortized cost of <i>i</i>th operation <i>(notice the 'hat')</i>. </li>
</ul> 
<p>Require &sum;<sub><i>i</i>=1,<i>n</i></sub><i> &ccirc;<sub>i</sub></i> &nbsp; &ge; &nbsp;
           &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i> for all sequences of <i>n</i>
   operations. That is, the difference between these sums always &ge; 0: we never owe anyone
   anything.</p> 

<!-- --------------- -->
<h3>Stack Example</h3>
<p>Whenever we <tt>Push</tt> an object (at actual cost of 1 cyberdollar), we potentially have to
pay CY$1 in the future to <tt>Pop</tt> it, whether directly or in <tt>Multipop</tt>.</p>
<p>To make sure we have enough money to pay for the <tt>Pops</tt>, we charge <tt>Push</tt> CY$2. </p>
<ul>
  <li>CY$1 pays for the push</li>
  <li>CY$1 is prepayment for the object being popped (metaphorically, this CY$1 is stored "on" the
      object).</li> 
</ul> 

<p>Since each object has CY$1 credit, the credit can never go negative.</p>

<img src="Topic-15/stack-amortized-cost-table.jpg">

<p>The total amortized cost <i>&ccirc;</i> = &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i> for
<em>any</em> sequence of <i>n</i> operations is an upper bound on the total actual cost <i>c</i> =
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i> for that sequence. </p>

<p>Since <i>&ccirc;</i> = O(<i>n</i>), also <i>c</i> = O(<i>n</i>). </p> 

<p>Note: we don't actually store cyberdollars in any data structures. This is just a metaphor to
enable us to compute an amortized upper bound on costs.</p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Potential Method (optional this semester)</h2>

<p>Instead of credit associated with objects in the data structure, this method uses the metaphor of
<i>potential</i> associated with the <i>entire data structure.</i></p>

<p>(I like to think of this as potential <i>energy,</i> but the text continues to use the monetary
metaphor.)</p>

<p>This is the most flexible of the amortized analysis methods, as it does not require maintaining
an object-to-credit correspondence.</p>

<p>Let </p>
<ul>
  <li>D<sub>0</sub> = initial data structure </li>
  <li>D<sub><i>i</i></sub> = data structure after <i>i</i>th operation</li>
  <li><i>c<sub>i</sub></i> = actual cost of <i>i</i>th operation. </li>
  <li><i>&ccirc;<sub>i</sub></i> = amortized cost of <i>i</i>th operation. </li>
</ul> 

<p>Potential Function <b>&Phi;</b>: D<sub><i>i</i></sub> &rarr; &real;, and we say that
&Phi;(D<sub><i>i</i></sub>) is the <b>potential</b> associated with data structure
D<sub><i>i</i></sub>. </p>

<p>We define the amortized cost <i>&ccirc;<sub>i</sub></i> to be the actual cost <i>c<sub>i</sub></i>
plus the change in potential due to the <i>i</i>th operation:</p>
<blockquote>
<i>&ccirc;<sub>i</sub></i> = <i>c<sub>i</sub></i> + &Phi;(D<sub><i>i</i></sub>) &minus; &Phi;(D<sub><i>i-1</i></sub>)
</blockquote>

<ul>
  <li>If at the <i>i</i>th operation, &Phi;(D<sub><i>i</i></sub>) &minus; &Phi;(D<sub><i>i-1</i></sub>) is
positive, then the amortized cost <i>c</i>'<sub><i>i</i></sub> is an <i>overcharge</i> and the
potential of the data structure increases.</li>

  <li>On the other hand, if &Phi;(D<sub><i>i</i></sub>) &minus; &Phi;(D<sub><i>i-1</i></sub>) is negative then
<i>c</i>'<sub><i>i</i></sub> is an undercharge, and the decrease of the potential of the data
structure pays for the difference (as long as it does not go negative). </li>
</ul>

<p>The total amortized cost across <i>n</i> operations is:</p>
<blockquote>
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i>
&nbsp; = &nbsp;
&sum;<sub><i>i</i>=1,<i>n</i></sub>(<i>c<sub>i</sub></i> + &Phi;(D<sub><i>i</i></sub>) -
&Phi;(D<sub><i>i-1</i></sub>))
&nbsp; = &nbsp;
(&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i>) + (&Phi;(D<sub><i>n</i></sub>) -
&Phi;(D<sub>0</sub>))
</blockquote>

<p>(The last step is taken because the middle expression involves a telescoping sum: every term other
than D<sub><i>n</i></sub> and D<sub>0</sub> is added once and subtracted once.)</p> 

<p>If we require that &Phi;(D<sub><i>i</i></sub>) &ge; &Phi;(D<sub>0</sub>) for all <i>i</i> then
the amortized cost will always be an upper bound on the actual cost no matter which <i>i</i>th step
we are on. </p>

<p>This is usually accomplished by defining &Phi;(D<sub>0</sub>) = 0 and then showing that
&Phi;(D<sub><i>i</i></sub>) &ge; 0 for all <i>i</i>. (Note that this is a constraint on &Phi;, not
on <i>&ccirc;</i>. <i>&ccirc;</i> can go negative as long as &Phi;(D<sub><i>i</i></sub>) never does.)</p>

<!-- --------------- -->
<h3>Stack Example</h3>

<p>Define &Phi;(D<sub><i>i</i></sub>) = number of objects in the stack.</p>

<p>Then &Phi;(D<sub>0</sub>) = 0 and &Phi;(D<sub><i>i</i></sub>) &ge; 0 for all <i>i</i>, since
there are never less than 0 objects on the stack.</p>

<p>Charge as follows (recalling that <i>&ccirc;<sub>i</sub></i> = <i>c<sub>i</sub></i> +
&Phi;(D<sub><i>i</i></sub>) - &Phi;(D<sub><i>i-1</i></sub>)):</p> 
<img src="Topic-15/stack-potential-table.jpg">

<p>Since we charge 2 for each <tt>Push</tt> and there are O(n) Pushes in the worst case, the
amortized cost of a sequence of <i>n</i> operations is O(<i>n</i>). </p>

<p>Does it seem strange that we charge <tt>Pop</tt> and <tt>Multipop</tt> 0 when we know they cost
something? </p>
<ul>
  <li> Remember that this is just a way of counting the total cost over a sequence of
       operations more precisely.</li>
  <li> It is not a claim about the actual cost of a specific procedural call.</li>
  <li> Like with the accounting method, we are guaranteeing that we have just enough credit on hand 
       to pay for the operations when they happen.</li>
  <li> The methods give a tight bound on amortized cost, but with much easier counting than if we
       had to reason about probability distributions, etc.</li> 
</ul> 

<!-- ------------------------------------------------------------ -->
<hr><h2>Application: Dynamic Tables</h2>

<p>There is often a tradeoff between time and space, for example, with hash tables. Bigger tables
give faster access but take more space. </p>
<p>Dynamic tables, like the Java Hashtable, grow dynamically as needed to keep the load factor
reasonable. </p>
<p>Reallocation of a table and copying of all elements from the old to the new table is
expensive!</p>

<p>But this cost is amortized over all the table access costs in a manner analogous to the stack
example: We arrange matters such that table-filling operations build up sufficient credit before
we pay the large cost of copying the table; so the latter cost is averaged over many operations.</p>

<!-- ----------------------------- -->
<h3>A Familiar Definition</h3>

<p><b>Load factor &alpha;</b> = <i>num</i>/<i>size</i>, where <i>num</i> = # items stored and
<i>size</i> = the allocated size of the table.</p>

<p>For the boundary condition of <i>size</i> = <i>num</i> = 0, we will define &alpha; = 1.</p>

<p>We never allow &alpha; &gt; 1 (no chaining).</p>

<!-- ----------------------------- -->
<h3>Insertion Algorithm</h3>

<p>We'll assume the following about our tables. (See Exercises 17.4-1 and 17.4-3 concerning
different assumptions.):</p>

<p>When the table becomes full, we double its size and reinsert all existing items. This guarantees
that &alpha; &ge; 1/2, so we are not wasting a lot of space. </p>
<pre>
Table-Insert (T,x)
1   if T.size == 0
2       allocate T.table with 1 slot 
3       T.size = 1
4   if T.num == T.size
5       allocate newTable with 2*T.size slots
6       insert all items in T.table into newTable
7       free T.table
8       T.table = newTable 
9       T.size = 2*T.size 
10  insert x into T.table 
11  T.num = T.num + 1
</pre> 

<p>Each <i>elementary insertion</i> has unit actual cost. Initially <i>T.num</i> = <i>T.size</i> =
0.</p>  

<!-- ----------------------------------- -->
<h3>Aggregate Analysis of Dynamic Table Expansion</h3> 

<p>Charge 1 per elementary insertion. Count only elementary insertions, since all other costs are
constant per call. </p>

<p><b><i>c<sub>i</sub></i></b> = actual cost of <i>i</i>th operation.</p>
<img src="Topic-15/pseudocode-table-insert.jpg" align="right">
<ul>
  <li> If the table is not full, <i>c<sub>i</sub></i> = 1 (for lines 1, 4, 10, 11). </li>
  <li> If full, there are <i>i</i> - 1 items in the table at the start of the <i>i</i>th
       operation. Must copy all of them (line 6), and then insert the <i>i</i>th item. Therefore 
       <i>c<sub>i</sub></i> = <i>i</i> - 1 + 1 = <i>i</i>. </li>
</ul>

<p>A sloppy analysis: In a sequence of <i>n</i> operations where any operation can be
O(<i>n</i>), the sequence of <i>n</i> operations is O(<i>n</i><sup>2</sup>). </p>

<p>This is "correct", but inprecise: we rarely expand the table! A more precise account of
<i>c<sub>i</sub></i>:</p>

<img src="Topic-15/c_i-definition.jpg">

<p>Then we can sum the total cost of all <i>c<sub>i</sub></i> for a sequence of <i>n</i> operations:</p>

<img src="Topic-15/analysis-table-expansion.jpg">
<img src="Topic-15/formula-A-5.jpg" align="right" border="1">

<p><em>Explain:</em> Why the <i>n</i>? What is the summation counting? Why does the summation start
at <i>j</i> = 0? Why does it end at <i>j</i> = lg <i>n</i>? </p>

<p>Therefore, the amortized cost per operation = 3: we are only paying a small constant price for
the expanding table.</p>

<!-- ------------------------------------ -->
<h3>Accounting Method Analysis of Dynamic Table Expansion</h3> 

<p>We charge CY$3 for each insertion into the table, i.e., the amortized cost of the <i>i</i>-th
insertion as <i>&ccirc;<sub>i</sub></i> = 3. The actual cost of the <i>i</i>-th insertion is as
before:</p>

<img src="Topic-15/c_i-definition.jpg">

<p> We must show that we never run out of credit. It's equivalent to proving the following Lemma.</p>

<p><b>Lemma</b>: <i>For any sequences of n insertions</i>
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i> &nbsp; &ge; &nbsp;
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i>. </p>

<p> <i>Proof</i>: By using the same equation as above, we know that for any <i>n</i>:
&sum;<sub><i>i</i>=1,<i>n</i></sub> <i>c<sub>i</sub></i> &le; 3<i>n</i>. By the definition of
&ccirc;, we know that: &sum;<sub><i>i</i>=1,<i>n</i></sub> <i>&ccirc;<sub>i</sub></i> =
3<i>n</i>. The lemma follows. &#x220E;</p>

<p>Intuitively, we charge CY$3 for inserting some item <i>x</i> into the table. Obviously, we are
overcharging for each <i>simple</i> insertion which costs only CY$1. However, the overcharging will
provide enough credit to pay for future copying of items when the table becomes full:

<ul>
  <li>The first CY$1 pays for the actual cost of inserting <i>x</i>.</li> 
  <li>The second CY$1 will pay for the cost of copying <i>x</i> into a new table the next time table becomes full. </li>
</ul>

<p> Note, the table might need to be expanded more than once after <i>x</i> is inserted, so <i>x</i>
might need to be copied more than once.  Who will pay for future copying of <i>x</i>? That's where
the third CY$1 comes in: </p>

<ul>
  <li>The third CY$1 will pay for the cost of copying some other item currently in the table that had been copied at least once before. </li> 
</ul>


<p> Let's see why CY$3 is enough to cover all possible future exansions and copying associated with them. </p>

<p> Suppose the capacity of the table is <i>m</i> immediately after an exansion. Then it holds
<i>m</i>/2 items and no item in the table contains any credits. For any insertion of an item
<i>x</i> we charge CY$3. CY$1 pays for the actual insertion of <i>x</i>; we place CY$1 credit on
<i>x</i> to pay for the cost of copying it in the future, and we place CY$1 credit on some other
item in the table that does not have any credit yet. We will have to insert <i>m</i>/2 items before
the next expansion of the table. Therefore, by the time the table will get expanded next time (and,
consequently, items need to be copied), every item of the table will have CY$1 credit associated
with it and this credit will pay for copying that item.

<p> The text also provides the potential analysis of table expansion.</p>

<p>This analysis assumed that the table never shrinks. See section 17.4 <!--(and your homework) -->
for an analysis using the potential method that covers shrinking tables.</p>


<!-- ------------------------------------------------------------ -->
<hr><h2>Other Examples</h2>

<p>Here are some other algorithms for which amortized analysis is useful:</p> 
<!-- --------------- -->
<h3>Red-Black Trees</h3>

<p>An amortized analysis of Red-Black Tree restructuring (Problem 17-4 of CLRS) improves upon our
analysis earlier in the semester:</p>

<ul>
  <li> Any sequence of <i>m</i> <tt>RB-Insert</tt> and <tt>RB-Delete</tt> operations performs
       O(<i>m</i>) structural modifications (rotations), </li>
  <li> Thus each operation does <b>O(1) structural modifications on average</b>, regardless of the size
       of the tree!</li>
  <li> An operation still may need to do O(lg <i>n</i>) recolorings, but these are very simple
       operations.</li>
</ul>

<!-- --------------- -->
<h3>Self-Organizing Lists</h3>

<ul> 
  <li> Self-organizing lists use a <b><i>move-to-front heuristic</i></b>: Immediately
after searching for an element, it is moved to the front of the list.</li>
  <li>This makes frequently accessed items more readily available near the front of
    the list.</li>
  <li> An amortized analysis (Problem 17-5) shows that the heuristic is no more than 4 times worse
  than optimal.</li>
</ul>

<!-- --------------- -->
<h3>Splay Trees</h3> 
<ul>
  <li>Splay trees are ordinary binary search trees (no colors, no height labels, etc.)</li>
  <li> After every access (every insertion, deletion, or search), the element operated on (or its
       parent in the case of deletion) is moved towards the top of the tree.</li>
  <li>This movement uses three <b><i>splaying</i></b> operations called "zig", "zig-zig" and "zig-zag".</li> 
  <li>Although in the worst case a splay tree can degenerate into an O(<i>n</i>) linked list, amortized
analysis shows that the expected case is O(lg <i>n</i>)</li>
  <li> Randomization can be used to make the worst case very unlikely.</li> 
  <li>If a single element is accessed at least <i>m</i>/4 times where <i>m</i>
is the number of operations, then the amortized running time of each of these accesses is O(1).</li>
  <li> Thus, splay-trees self-organize to provide fast access to frequently accessed items.</li>
  <li> This makes them good
for locality of reference in memory, but multithreaded access must be implemented carefully.</li>
</ul> 

<!-- --------------- -->
<h3>To Be Continued </h3>
<p>Amortized analysis will be used in analyses of</p>
<ul>
  <li>Graph search (Topic 14, Ch. 22) </li>
  <li>Disjoint set operations (Topic 16, Ch. 21) </li>
  <li>Dijkstra's Algorithm for Shortest Paths (Topic 18, Ch. 24) </li> 
</ul>

<!-- ------------------------------------------------------------ -->
<hr>
<address>Dan Suthers &amp; Nodari Sitchinava</address>
<!-- hhmts start -->Last modified: Mon Mar 30 13:17:21 HST 2015 <!-- hhmts end -->
<br>Images are from the instructor's material for Cormen et al. Introduction to Algorithms, Third
Edition.</br> 
</body>
</html>
