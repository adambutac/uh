<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<title>ICS 311 #8: Binary Search Trees </title>
</head>

<body>

<hr><h1><a href="../index.html">ICS 311</a> #8: Binary Search Trees </h1><hr> 

<!-- ------------------------------------------------------------ -->
<h2>Outline</h2> 
<ol>
  <li> Trees, Binary Trees, Binary Search Trees </li>
  <li> Querying BSTs</li>
  <li> Modifying BSTs (Insertion and Deletion)</li>
  <li> Performance of BSTs </li>
</ol>

<h2>Readings and Screencasts</h2>
<ul>
  <li>CLRS 3rd Ed. Sections 12.1-12.3</li>
  <li>Also skim 12.4 to gain an appreciation of how the methods we have been learning (indicator random
variables, recurrence relations, and proof by substitution) can all be applied together in a
substantial proof of an important fact: the expected height of randomly built binary search
trees. </li>
  <li> The present web page has additional material on types of trees and quantitative facts that are
not found in Chapter 12, so read at least the beginning of these notes too.</li>
  <li>Screencasts <a href="http://youtu.be/bAxzRuu3Uy4">8A</a>,
                  <a href="http://youtu.be/LDncFcNOr_I">8B</a>,
                  <a href="http://youtu.be/qJ7TyaKSf_0">8C</a>,
                  <a href="http://youtu.be/vx0uWYHIRes">8D</a>
                  (also in Laulima and iTunesU)</li>
</ul> 

<!-- ------------------------------------------------------------ -->
<hr><h2> Trees, Binary Trees, Binary Search Trees </h2>

<p>First, a preliminary look at trees. (This should be review. Some of this material is taken from
Thomas Standish Data Structure Techniques (1980) and Goodrich & Tamassia (1998) as well as the
Cormen appendix, but is widely published.)</p>

<!-- ------------ -->
<a name="freetrees">
<h3> Fundamental Theorem of Free Trees </h3> </a> 
<p> If <i>G</i>=(<i>V</i>,<i>E</i>) is a finite graph with <i>v &gt; 1</i> vertices, the
following properties are equivalent definitions of a generalized or <b>free tree</b>:</p>
<ol>
  <li> <i>G</i> is connected and has no simple cycles. </li>
  <li> <i>G</i> has no simple cycles and has <i>v-1</i> edges (|<i>E</i>| = |<i>V</i>| - 1)</li>
  <li> <i>G</i> is connected and has <i>v-1</i> edges.</li>
  <li> <i>G</i> is acyclic, and if an edge is added that joins two nonadjacent vertices, exactly
       one cycle is formed.</li>
  <li> <i>G</i> is connected, but if an edge is deleted, <i>G</i> becomes disconnected. </li>
  <li> Every pair of vertices is connected by exactly one path. </li>
</ol>

<p>Although this is a definition, the theorem is that these
definitions are equivalent. A classic exercise in basic graph theory is to prove each of these
statements using the one before it, and #1 from #6.</p>

<h4>Comments</h4>

<p>When we use the term "tree" without qualification, we will assume
that we mean a free tree unless the context makes it clear otherwise
(e.g., when we are discussing binary trees).</p> 

<p>In some contexts, <i>G</i>=({},{}) and <i>G</i>=({<i>v</i>},{}) are also treated as trees. These are obvious
base cases for recursive algorithms.</p> 

<p>A <b>forest</b> is a (possibly disconnected) graph, each of whose connected components is a
tree.<p>

<p>An <b>oriented tree</b> is a directed graph having a designated vertex <i>r</i>, called the
<b>root</b>, and having exacly one oriented path between the root and any vertex <i>v</i> distinct
from the root, in which <i>r</i> is the origin of the path and <i>v</i> the terminus.</p>

<p>In some fields (such as social network analysis), the word "node" is used interchangeably with
"vertex". I use "vertex" in these notes but may slip into "node" in my recorded lectures or in class. </p> 

<!-- ------------- -->
<img src="Topic-08/diagram-tree-heights-tall.jpg" align="right" border="1">
<h3><a name="binarytrees">Binary Trees</a> </h3>

<p>A <b>binary tree</b> is a finite set of vertices that is either empty or consists of a
vertex called the root, together with two binary subtrees that are disjoint from each other and from
the root and are called the <b>left</b> and <b>right subtrees</b>.</p>

<p>A <b>full binary tree </b> is a binary tree in which each vertex either is a leaf or has exactly
two nonempty descendants. In a full binary tree of height <i>h</i>:</p>

<ol>
  <li> # leaves = (# internal vertices) + 1.</li>
  <li> # leaves is at least <i>h</i>+1 <i>(first example figure)</i> and at most
       2<sup><i>h</i></sup>  <i>(second example figure)</i>.</li>
  <li> # internal vertices is at least <i>h</i> <i>(first example)</i> and at most
       2<sup><i>h</i></sup>-1 <i>(second example)</i>.</li>
  <li> Total number of vertices (summing the last two results) is at least 2<i>h</i>+1 <i>(first
       example)</i> and at most 2<sup><i>h+1</i></sup>-1 <i>(second example)</i>.</li>
       <img src="Topic-08/diagram-tree-heights-wide.jpg" align="right" border="1">  
  <li> Height <i>h</i> is at least lg(<i>n</i>+1)-1 <i>(second example)</i> and at most
       (<i>n</i>-1)/2 <i>(first example)</i></li>  
</ol> 

<p>A <b>complete binary tree </b> is full binary tree in which all leaves have the same depth and
all internal vertices have degree 2 <i>(e.g., second example above)</i>.</p>

<p>(<i>Note:</i> some earlier texts allow the last level of a "complete" tree to be incomplete!
They are defined as binary trees with leaves on at most two adjacent levels <i>l-1</i> and <i>l</i>
and in which the leaves at the bottommost level <i>l</i> lie in the leftmost positions of
<i>l</i>.)</p>

<!-- -------------------------- -->
<h3>Binary Search Trees (BSTs)</h3>
<p>A <b>binary search tree</b> (BST) is a binary tree that satisfies the <b>binary search tree
property:</b>
<ul>
  <li> if <i>y</i> is in the left subtree of <i>x</i> then <i>y.key &le; x.key</i>. </li>
  <li> if <i>y</i> is in the right subtree of <i>x</i> then <i>y.key &ge; x.key</i>. </li>
</ul> 
<p>BSTs provide a useful implementation of the Dynamic Set ADT, as they support most of the
  operations efficiently (as will be seen).</p>

<p>Two examples on the same data:<br>
<img src="Topic-08/Fig-12-1a-balanced.jpg">
<img src="Topic-08/Fig-12-1b-unbalanced.jpg">
</p>

<p><i>Could we just just say "if y is the <b>left child</b> of x then
y.key &le; x.key, etc., and rely on transitivity? What would go wrong?</i></p>

<p> Implementations of BSTs include a <i>root</i> instance variable. Implementations of BST vertices
  usually include fields for the <i>key</i>, <i>left</i> and <i>right</i> children, and the
  <i>parent</i>.

<!-- ------------------------------------------------------------ -->
<hr><h2>Querying Binary Search Trees </h2>

<p>Note that all of the algorithms described here are given a tree vertex as a starting point. Thus,
they can be applied to any subtree of the tree as well as the full tree.</p>

<h3>Traversing Trees</h3>
<p>Traversals of the tree "visit" (e.g., print or otherwise operate on) each vertex of the tree
exactly once, in some systematic order. This order can be <b>Inorder</b>, <b>Preorder</b>, or
<b>Postorder</b>, according to when a vertex is visited relative to its children. Here is the code for
inorder:</p> 

<img src="Topic-08/pseudocode-inorder-tree-walk.jpg">
<p> <i>Quick exercise: Do INORDER-TREE-WALK on this tree ... in what order are the keys
printed?</i></p>  
<img src="Topic-08/example-BST-simple.jpg">
<p><i>Quick exercise: How would you define Preorder traversal? Postorder traversal?</i></p>

<p> Traversals can be done on any tree, not just binary search trees. For example, traversal of an
expression tree will produce preorder, inorder or postorder versions of the expressions.</p>

<h4>Time to Traverse a BST</h4>

<p><b>Time:</b> Traversals (INORDER-TREE-WALK and its preorder and postorder variations) take
<i>T</i>(<i>n</i>) = &Theta;(<i>n</i>) time for a tree with <i>n</i> vertices, because we visit and
print each vertex once, with constant cost associated with moving between vertices and printing
them. More formally, we can prove as follows:</p>

<p><i>T</i>(<i>n</i>) = &Omega;(<i>n</i>) since these traversals must visit all <i>n</i> vertices of
the tree.</p> 

<p><i>T</i>(<i>n</i>) = O(<i>n</i>) can be shown by substitution. First the base case of the
recurrence relation captures the work done for the test <i>x</i> &ne; NIL:</p>

<blockquote>
<i>T</i>(0) = <i>c</i> for some constant c &gt; 0
</blockquote>

<p> To obtain the recurrence relation for <i>n</i> &gt; 0, suppose the traversal is called on a vertex
<i>x</i> with <i>k</i> vertices in the left subtree and <i>n</i>&minus;<i>k</i>&minus;1 vertices in the
right subtree, and that it takes constant time <i>d</i> &gt; 0 to execute the body of the traversal
exclusive of recursive calls. Then the time is bounded by</p>

<blockquote>
<i>T</i>(<i>n</i>) &le; <i>T</i>(<i>k</i>) + <i>T</i>(<i>n</i>&minus;<i>k</i>&minus;1) +
<i>d</i>. 
</blockquote>

<p>We now need to "guess" the inductive hypothesis to prove. The "guess" that CLRS use is
<i>T</i>(<i>n</i>) &le; (<i>c</i> + <i>d</i>)<i>n</i> + <i>c</i>, which is clearly O(<i>n</i>).
It's less clear how they got this guess. As discussed in Chapter 4, section 4 (especially subsection
"Subtleties" page 85-86), one must prove the exact form of the inductive hypothesis, and sometimes
you can get a better guess by observing how your original attempt at the proof fails. Perhaps this
is what they did.  We'll skip the failure part and go directly to proving their hypothesis by
substitution (showing two steps skipped over in the book):</p>

<blockquote>
<b><i>Inductive hypothesis:</i></b> Suppose that <i>T</i>(<i>m</i>) &le; (<i>c</i> +
<i>d</i>)<i>m</i> + <i>c</i> for all <i>m</i> &lt; <i>n</i><br><br> 

<b><i>Base Case:</i></b> (<i>c</i> + <i>d</i>)0 + <i>c</i> = <i>c</i> = <i>T</i>(0) as defined above.<br><br>

<b><i>Inductive Proof:</i></b><br>

&nbsp; &nbsp;<i>T</i>(<i>n</i>) &le; <i>T</i>(<i>k</i>) + <i>T</i>(<i>n</i>&minus;<i>k</i>&minus;1) +
<i>d</i> &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp;
<i>by definition</i> <br>

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= ((<i>c</i> + <i>d</i>)<i>k</i> + <i>c</i>) + 
((<i>c</i> + <i>d</i>)(<i>n</i>&minus;<i>k</i>&minus;1) + <i>c</i>) + <i>d</i>
&nbsp; &nbsp;<i>substiting inductive hypothesis for values &lt; n</i> <br>

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = ((<i>c</i> + <i>d</i>)(<i>k</i> + <i>n</i> &minus;
<i>k</i> &minus; 1) + <i>c</i> + <i>c</i> + <i>d</i> 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <i>collecting factors </i> <br>


&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = ((<i>c</i> + <i>d</i>)(<i>n</i> &minus; 1) + <i>c</i> +
<i>c</i> + <i>d</i>  
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <i>simplifying  </i> <br>

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = ((<i>c</i> + <i>d</i>)<i>n</i> + <i>c</i> &minus;
(<i>c</i> + <i>d</i>) + <i>c</i> + <i>d</i>  
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
<i>multiplying out <i>n</i>&minus;1 and rearranging  </i> <br>

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = ((<i>c</i> + <i>d</i>)<i>n</i> + <i>c</i>.
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
<i>the last terms cancel.</i> 

</blockquote>

<!-- ---------------------------------- -->
<h3>Searching for an Element in a BST </h3>
<p>Here are two implementations of the dynamic set operation <tt>search</tt>: </p>

<img src="Topic-08/pseudocode-recursive-tree-search.jpg">
<img src="Topic-08/pseudocode-iterative-tree-search.jpg">
<p><i>Quick exercise: Do TREE-SEARCH for D and C on this tree ... </i></p> 
<img src="Topic-08/example-BST-simple.jpg">

<p>For now, we will characterize the run time of the remaining algorithms in terms of <i>h</i>, the
height of the tree. Then we will consider what <i>h</i> can be as a function of <i>n</i>, the number
of vertices in the tree. </p>

<p><b>Time:</b> Both of the algorithms visit vertices on a downwards path from the root to the vertex
sought. In the worst case, the leaf with the longest path from the root is reached, examining
<i>h</i>+1 vertices (<i>h</i> is the height of the tree, so traversing the longest path must
traverse <i>h</i> edges, and <i>h</i> edges connect <i>h</i>+1 vertices). Comparisons and movements
to the chosen child vertex are O(1), so the algorithm is O(<i>h</i>). (<i>Why don't we say
&Theta;?</i>) </p>

<!-- ---------------------------------------- -->
<h3>Finding the Minimum and Maximum Element </h3>
<p>The BST property guarantees that:</p>
<ul>
  <li>The minimum key of a BST is located at the leftmost vertex.</li>
  <li>The maximum key of a BST is located at the rightmost vertex.</li>
</ul>
<p><i>(Why?)</i> This leads to simple implementations:</p> 
<img src="Topic-08/pseudocode-tree-min-max.jpg">
<img src="Topic-08/example-BST-simple.jpg">

<p><b>Time:</b> Both procedures visit vertices on a path from the root to a leaf. Visits are O(1), so
again this algorithm is O(<i>h</i>).</p>

<!-- ------------------------------------------------ -->
<h3> Finding the Successor or Predecessor of an Element </h3>

<p>Assuming that all keys are distinct, the successor of a vertex <i>x</i> is the vertex <i>y</i> such
that <i>y.key</i> is the smallest <i>key</i> &gt; <i>x.key</i>. If <i>x</i> has the largest key in the BST,
we define the successor to be NIL.</p>

<p>We can find <i>x</i>'s successor based entirely on the tree structure (no key comparison is
needed). There are two cases:</p>

<ol>
  <li><b>If vertex <i>x</i> has a non-empty right subtree, then <i>x</i>'s successor is the minimum in
      its right subtree.</b> <i>(Why?)</i></li> 
  <li><b>If vertex <i>x</i> has an empty right subtree, then <i>y</i> is the lowest ancestor of <i>x</i>
  whose left child is also an ancestor of <i>x</i>.</b> &nbsp; <i>To see this, consider these facts: </i>
      <ul>
	<li> If <i>y</i> is the successor of <i>x</i> then <i>x</i> is the predecessor of <i>y</i>,
             so <i>x</i> is the maximum in <i>y</i>'s left subtree <i>(flip the reasoning of your
             answer to the last question)</i>.</li>
       <li> Moving from <i>x</i> to the left up the tree (up through right children) reaches vertices 
             with smaller keys, which must also be in this left subtree. </li>
      </ul> </li> 
</ol>

<img src="Topic-08/pseudocode-tree-successor.jpg">
<p><i>Exercise: Write the pseudocode for TREE-PREDECESSOR</i></p>

<p> Let's trace the min, max, successor (15, 13, 6, 4), and predecessor (6) operations:</p> 
<img src="Topic-08/Fig-12-2-example-BST.jpg">

<p><b>Time:</b> The algorithms visit notes on a path down or up the tree, with O(1) operations at
each visit and a maximum of <i>h+1</i> visitations. Thus these algorithms are O(<i>h</i>). </p>

<p><i>Exercise: Show that if a vertex in a BST has two children, then its succesor has no left
child and its predecessor has no right child.</i></p>

<!-- ------------------------------------------------------------ -->
<hr><h2> Modifying Binary Search Trees </h2>
<p>The key point is that the BST property must be sustained. This is more straightforward with
insertion (as we can add a vertex at a leaf position) than with deletion (where an internal vertex may
be deleted). </p>

<!-- ----------- -->
<h3> Insertion </h3>

<p>The algorithm assumes that the vertex <i>z</i> to be inserted has been initialized with
 <i>z.key</i> = <i>v</i> and <i>z.left</i> = <i>z.right</i> = NIL.</p>

<p>The strategy is to conduct a search (as in tree search) with pointer <i>x</i>, but to sustain a
<b>trailing pointer</b> <i>y</i> to keep track of the parent of <i>x</i>. When <i>x</i> drops
off the bottom of the tree (becomes NIL), it will be appropriate to insert <i>z</i> as a
child of <i>y</i>.</p>

<p>Comment on variable naming: I would have preferred that they call <i>x</i> something like
<tt>leading</tt> and <i>y</i> <tt>trailing</tt>.</p> 

<img src="Topic-08/pseudocode-tree-insert.jpg">

<p>Try <tt>TREE-INSERT(T,C)</tt>:</p> 
<img src="Topic-08/example-BST-simple.jpg">

<p><b>Time:</b> The same as TREE-SEARCH, as there are just a few additional lines of O(1) pointer
manipulation at the end.</p>

<p><i>Discuss: How would you use TREE-INSERT and INORDER-TREE-WALK to sort a set of numbers?</i><br>
<i>Think about at home: How would you prove its time complexity?</i></p>

<!-- ---------- -->
<a name="deletion"><h3> Deletion </h3></a> 

<p>Deletion is more complex, as the vertex <i>z</i> to be deleted may or may not have children. We can
think of this in terms of three cases: </p>
<ol>
  <li>If <i>z</i> has no children, we can just remove it (by setting <i>z</i>'s parent's pointer to NIL). </li>
  <li>If <i>z</i> has just one child <i>c</i>, then make <i>c</i> take <i>z</i>'s position in the
      tree, updating <i>z</i>'s parent to point to <i>c</i> and "dragging" <i>c</i>'s subtree
      along.</li> 
  <li>If <i>z</i> has two children, find <i>z</i>'s successor <i>y</i> and replace <i>z</i> by
      <i>y</i> in the tree (noting that <i>y</i> has no left child):
      <ul>
        <li>If <i>y</i> is <i>z</i>'s right child, then replace <i>z</i> by <i>y</i> (including
            updating <i>z</i>'s parent to point to <i>y</i>, and <i>y</i> to point to <i>z</i>'s
            left child) and we are done. </li> 
        <li>Otherwise <i>y</i> is further down in <i>z</i>'s right subtree (and again has no left
            child): 
            <ol> 
               <li>Replace <i>y</i> with its own right child. </li>
               <li>The rest of <i>z</i>'s right subtree becomes <i>y</i>'s new right subtree.</li>
               <li><i>z</i>'s left subtree becomes <i>y</i>'s new left subtree.</li>
               <li>Make <i>z</i>'s parent point to <i>y</i>.</li> 
           </ol> 
        </li>
      </ul></li>
</ol> 

<p>The code organizes the cases differently to simplify testing and make use of a common procedure
for moving subtrees around. This procedure replaces the subtree rooted at <i>u</i> with the subtree
rooted at <i>v</i>. </p>

<ul>
  <li> It makes <i>u</i>'s parent become <i>v</i>'s parent (lines 6-7), unless <i>u</i> is the root,
  in which case it makes <i>v</i> the root (lines 1-2).</li>
  <li> <i>v</i> replaces <i>u</i> as <i>u</i>'s parent's left or right child (lines 3-5).</li>
  <li> It does not update <i>v.left</i> or <i>v.right</i>, leaving that up to the caller.
</ul> 
<img src="Topic-08/pseudocode-transplant.jpg">
<p><i>(If we have time, draw a few examples.)</i></p>

<p>Here are the four actual cases used in the main algorithm TREE-DELETE(T,<i>z</i>):</p>

<img src="Topic-08/Fig-12-4-a-no-left-child.jpg" align="right">
<h4>No left child (and possibly no children):</h4>
<p>If <i>z</i> has no left child, replace <i>z</i> by its right child (which may or may not be
NIL). This handles case 1 and half of case 2 in the conceptual breakdown above. (Lines 1-2 of final algorithm.)</p>

<img src="Topic-08/Fig-12-4-b-no-right-child.jpg" align="right">
<h4>No right child (and has left child):</h4>
<pk>If <i>z</i> has just one child, and that is its left child, then replace <i>z</i> by its left
child. This handles the rest of case 2 in the conceptual breakdown above. (Lines 3-4.)</p> 

<p>Now we just have to deal with the case where both children are present. Find <i>z</i>'s successor
(line 5), which must lie in <i>z</i>'s right subtree and have no left child (<i>why?</i>). Handling
depends on whether or not the successor is immediately referenced by <i>z</i>:</p>

<img src="Topic-08/Fig-12-4-c-successor-is-child.jpg" align="right">
<h4>Successor is child:</h4>

<p>If successor <i>y</i> is <i>z</i>'s right child (line 6), replace <i>z</i> by <i>y</i>, "pulling
up" <i>y</i>'s right subtree. The left subtree of <i>y</i> is empty so we can make <i>z</i>'s former
left subtree <i>l</i> be <i>y</i>'s new left subtree. (Lines 10-12.)</p>

<h4>Successor is not child:</h4>
<p>Otherwise, <i>y</i> is within <i>z</i>'s right subtree rooted at <i>r </i>but is not the root of this
subtree (<i>y&ne;r</i>).</p>
<ol>
  <li>Replace <i>y</i> by its own right child <i>x</i>. (Line 7.)</li>
  <li>Set <i>y</i> to be <i>r</i>'s parent. (Line 8-9.)</li>
  <li>Then let <i>y</i> take <i>z</i>'s place with respect to <i>z</i>'s parent <i></i> and left
      child <i>l</i>. (Lines 10-12.)</li>
</ol> 
<img src="Topic-08/Fig-12-4-d-successor-not-child.jpg">

<p>Now we are ready for the full algorithm:</p> 
<img src="Topic-08/pseudocode-tree-delete.jpg">
<p>The last three lines excecute whenever <i>z</i> has two children (the last two cases above).</p> 

<p>Let's try <tt>TREE-DELETE(T,<i>x</i>)</tt> on <i>x=</i> I, G, K, and B:</p> 
<img src="Topic-08/example-BST-to-delete.jpg">

<p><b>Time:</b> Everything is O(1) except for a call to TREE-MINIMUM, which is O(<i>h</i>), so
TREE-DELETE is O(<i>h</i>) on a tree of height <i>h</i>. </p>

<p>The above algorithm fixes a problem with some published algorithms, including the first two
editions of the book. Those versions copy data from one vertex to another to avoid a tree
manipulation. If other program components maintain pointers to tree vertices (or their positions in
Goodrich & Tamassia's approach), this could invalidate their pointers. The present version
guarantees that a call to TREE-DELETE(T, <i>z</i>) deletes exactly and only vertex <i>z</i>.</p>

<p>An animation is available at <a
href="http://www.csc.liv.ac.uk/~ullrich/COMP102/applets/bstree/">http://www.csc.liv.ac.uk/~ullrich/COMP102/applets/bstree/</a>
(The code shown probably has the flaw discussed above.)</p> 

<!-- ------------------------------------------------------------ -->
<hr><h2> Performance of Binary Search Trees </h2>

<p>We have been saying that the asympotic runtime of the various BST operations (except traversal)
are all O(lg <i>h</i>), where <i>h</i> is the height of the tree. But <i>h</i> is usually hidden
from the user of the ADT implementation and we are more concerned with the runtime as a function of
<i>n</i>, our input size. So, what is <i>h</i> as a function of <i>n</i>? </p>

<p>We know that in the worst case, <i>h</i> = O(<i>n</i>) (when the tree degenerates to a linear
chain). Is this the expected case? Can we do anything to guarantee better performance? These two
questions are addressed below.</p>

<h3> Expected height of randomly built binary search trees</h3> 

<p> The textbook has a proof in section 12.4 that <b>the expected height of a randomly build binary search
tree on <i>n</i> distinct keys is O(lg <i>n</i>).</b></p> 

<p> We are not covering the proof (and you are not expected to know it), but I recommend reading it,
as the proof elegantly combines many of the ideas we have been developing, including indicator random
variables and recurrences. (They take a huge step at the end: can you figure out how the log of the
last polynomial expression simplifies to O(lg <i>n</i>)?)</p>

<p> An alternative proof provided by Knuth (Art of Computer Programming Vol. III, 1973, p 247), and
also summarized by Standish, is based on average path lengths in the tree. It shows that about
1.386 lg <i>n</i> comparisons are needed: <b>the average tree is about 38.6% worse than the best
possible tree in number of comparisons required for average search</b>. </p>

<p>Surprisingly, <u>analysts have not yet been able to get clear results when random deletions are also
included</u>. </p> 

<h3>Balanced Trees</h3>
<p>Given the full set of keys in advance, it is possible to build an optimally balanced BST for
those keys (guaranteed to be lg <i>n</i> height). See section 15.5 of the Cormen et al. text.</p>

<p>If we don't know the keys in advance, many clever methods exist to keep trees balanced, or
balanced within a constant factor of optimal, by performing manipulations to re-balance after
insertions (AVL trees, Red-Black Trees), or after all operations (in the case of splay trees). We
cover Red-Black Trees in two weeks (<a href="Topic-11.html">Topic 11</a>), after a diversion to
heaps (which have tree-like structure) and sorting.</p>

<!-- ------------------------------------------------------------ -->
<hr><h2>Next</h2>

<p>In <a href="Topic-09.html">Topic 09</a> we look at how a special
kind of tree, a Heap, can be embedded in an array and used to
implement a sorting algorithm and priority queues.</p>

<p>After a brief diversion to look at other sorting algorithms, we
will return to other kinds of trees, in particular special kinds of
binary search trees that are kept balanced to guarantee O(lg <i>n</i>)
performance, in <a href="Topic-11.html">Topic 11</a>. </p>

<!-- ------------------------------------------------------------ -->
<hr>
<address>Dan Suthers</address>
<!-- hhmts start -->Last modified: Mon Sep 14 21:49:32 HST 2015 <!-- hhmts end -->

<br>Images are from the instructor's material for Cormen et al. Introduction to Algorithms, Third
Edition.</br>

</body>
</html>
